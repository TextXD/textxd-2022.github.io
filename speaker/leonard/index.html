<!doctype html><html lang=en-us><head><meta charset=utf-8><title>Peter Leonard</title><meta name=viewport content="width=device-width,initial-scale=1,maximum-scale=1"><meta name=description content="This is meta description"><meta name=author content="Textxd"><meta name=generator content="Hugo 0.107.0"><link rel=stylesheet href=https://examplesite.org/plugins/bootstrap/css/bootstrap.min.css><link rel=stylesheet href=https://examplesite.org/plugins/themefisher-font/themefisher-font.min.css><link rel=stylesheet href=https://examplesite.org/scss/style.min.css media=screen><link rel="shortcut icon" href=https://examplesite.org/images/favicon.png type=image/x-icon><link rel=icon href=https://examplesite.org/images/favicon.png type=image/x-icon></head><body><div class=preloader></div><header class=header-bar><nav class="navbar navbar-expand-lg main-nav"><div class=container><a class=navbar-brand href=https://examplesite.org><img src=https://examplesite.org/images/logo.png alt=TextXD class="img-fluid logo-b"></a>
<button class=navbar-toggler type=button data-toggle=collapse data-target=#navigation aria-controls=navigation aria-expanded=false aria-label="Toggle navigation">
<span class=tf-ion-android-menu></span></button><div class="collapse navbar-collapse text-center" id=navigation><ul class="navbar-nav mx-auto"><li class=nav-item><a class=nav-link href=https://examplesite.org>Home</a></li><li class=nav-item><a class=nav-link href=https://examplesite.org/about>About</a></li><li class="nav-item dropdown"><a class="nav-link dropdown-toggle" href=# role=button data-toggle=dropdown aria-haspopup=true aria-expanded=false>Pages</a><div class=dropdown-menu><a class=dropdown-item href=https://examplesite.org/faq>FAQ</a>
<a class=dropdown-item href=https://examplesite.org/schedule>Schedule</a>
<a class=dropdown-item href=https://examplesite.org/pricing>Tickets</a></div></li><li class=nav-item><a class=nav-link href=https://examplesite.org/contact>Contact</a></li></ul></div></div></nav></header><section class=page-header style=background-image:url(https://examplesite.org/images/speakers/profile.png),url(https://examplesite.org/images/bg/cta-bg.jpg)><div class=overly></div><div class=container><div class="row justify-content-center"><div class=col-lg-8><div class="content text-center"><h1 class="mb-3 text-white text-capitalize letter-spacing">Peter Leonard</h1><div class="divider mx-auto mb-4 bg-white"></div><ul class=list-inline><li class=list-inline-item><a href=https://examplesite.org>Home</a> /</li><li class=list-inline-item>Peter Leonard</li></ul></div></div></div></div></section><section class="speaker-single section"><div class=container><div class=row><div class="col-lg-6 col-md-5 mb-5 mb-md-0"><img src=https://examplesite.org/images/speakers/profile.png alt="Peter Leonard" class="img-fluid w-100"></div><div class="col-lg-6 col-md-7"><div class=speaker-single-wrap><div class="speaker-header mb-4"><h2>Peter Leonard</h2><span class=text-color>Stanford Libraries</span></div><h3>Text & visual cultural heritage collections: evocative possibilities</h3><p>Large-scale digitized cultural heritage collections, consisting of hundreds of thousands of pictures that are not reducible to text alone, are a growing site of digital research practice. (Wevers & Smits, 2020). Additionally, APIs and standards such as the International Image Interoperability Framework (IIIF) have made possible work at more granular levels – bounding boxes determined by curators, convolutional neural networks, or a combination of human and machine intelligence. It is now possible to produce sub-corpora of illustrated “initial” letters from medieval manuscripts, or faces from 20th century photographs, creating highly specialized image datasets out of larger materials. Enhancing information retrieval of these visual datasets remains a difficult problem, especially in digital search systems that are text-centric. How would a library or museum search engine return relevant results among thousands of undescribed illustrations or unlabeled photographs? One answer is ‘visual similarity’, or computation of cosine distance an embedding space determined by the semifinal layer of a captioning CNN: EPFL’s Replica, the Norwegian State Library’s Maken experiment, among others. But the image similarity method does not solve the initial problem of having an initial image from which to gauge similarity. Given hundreds of thousands of unlabeled images, how does one know where to start? One answer may lie in leveraging text-to-image networks, such as CLIP (Contrastive Language-Image Pre-Training). These architectures generally seek to predict the most likely text section (word or phrase) given an unseen image. Although most commonly thought of in image-generation contexts (DALL-E; various Diffusion models), they can also be used to generate descriptions of visual information that are bracingly unmoored from the scholarly or archival description practices commonly used in the GLAM sector. For good and for ill, they can make image search systems responsive to evocative phrases such as “a man alone on a road at night”, or “I am feeling cold.” (See https://huggingface.co/spaces/NbAiLab/maken-clip-text for a working example from the Norwegian State Library). Important caveats remain: these models may be doubly affected by bias and incomplete data, at both their linguistic and visual ends. They are certainly dually anachronistic in the same ways, assuming the images in question are not contemporary pictures. And questions of reproducibility and integration into existing search methodologies have only begun to be explored. Nevertheless, given the astounding scale of mass digitization projects currently underway, it seems prudent to examine this frontier of “evocative” text search across visual collections. Doing so may re-center text as a human search methodology with a long and well-studied history, and even create the potential for more rewarding results for searchers who lack access to the precise terminology used to describe elements of our common visual cultural heritage. This talk will examine the results of such CLIP-based search models on image datasets, including examples drawn from the Meserve-Kunhardt collection of 19th century American photography, images in Vogue magazine (1892-2013), and the photography of Andy Warhol.</p><p><b>Bio:</b></p><div class=content></div><h5 class="mb-3 mt-5"></h5><ul class="list-inline social-single"></ul></div></div></div></div></section><footer class="footer section"><div class=container><div class="row justify-content-center"><div class="col-lg-6 text-center"><h2 class="text-white mb-3">TextXD</h2><p class=text-white-50>Text Analysis across domains</p><ul class="list-inline footer-socails"></ul></div></div><div class=row><div class="col-lg-12 text-center mt-5"><p class="copy border-top pt-4 text-white-50 mb-0"></p></div></div></div></footer><script>var indexURL="https://examplesite.org/index.json"</script><script src></script>
<script src=https://examplesite.org/plugins/jquery/jquery.js></script>
<script src=https://examplesite.org/plugins/bootstrap/js/bootstrap.min.js></script>
<script src=https://examplesite.org/plugins/syotimer/syotimer.min.js></script>
<script src=https://examplesite.org/plugins/search/fuse.min.js></script>
<script src=https://examplesite.org/plugins/search/mark.js></script>
<script src=https://examplesite.org/plugins/search/search.js></script>
<script src=https://examplesite.org/plugins/google-map/map.js></script>
<script src=https://examplesite.org/js/script.min.js></script></body></html>